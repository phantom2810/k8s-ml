<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Deploying machine learning models to Kubernetes</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deploying machine learning models to Kubernetes</h1>
</header>
<h1 id="deploy-a-machine-learning-model-on-kubernetes">Deploy a machine
learning model on Kubernetes</h1>
<p>This sequence of exercises will introduce you to some basic
principles of cloud computing, in the context of deploying a machine
learning model to the cloud. You will learn the following:</p>
<p><em>Deploy an image classification as a web service</em>. After
completing this section, you should be able to:</p>
<ul>
<li>describe advantages of deploying a web service using containers</li>
<li>incorporate a machine learning model into a web application using
Flask</li>
<li>build, deploy, and administer containers to serve a web
application</li>
</ul>
<p><em>Deploy a service using container orchestration (Kubernetes)</em>.
After completing this section, you should be able to:</p>
<ul>
<li>describe benefits of using a container orchestration framework,
versus directly deploying containers</li>
<li>configure, deploy, and test a service deployed on a Kubernetes
cluster</li>
</ul>
<p><em>Deploy a load balanced service on Kubernetes</em>. After
completing this section, you should be able to:</p>
<ul>
<li>explain tradeoffs of load balancing with respect to service time and
resource usage</li>
<li>configure, deploy, and test a load balanced service on
Kubernetes</li>
</ul>
<p><em>Deploy a service with dynamic scaling</em>. After completing this
section, you should be able to:</p>
<ul>
<li>explain tradeoffs of dynamic scaling with respect to service time
and resource usage</li>
<li>configure, deploy, and test a service with dynamic scaling on
Kubernetes</li>
</ul>
<p>This sequence assumes that you have already deployed a Kubernetes
cluster on Chameleon, using <a
href="https://chameleoncloud.org/experiment/share/9bae9a8a-68fa-402c-ae51-41431eb78732">this
recipe</a>, and that you can SSH into the primary node in your
cluster.</p>
<p>On the primary node in your cluster, “node-0”, run</p>
<pre><code>git clone --single-branch --branch gh-pages https://github.com/teaching-on-testbeds/k8s-ml.git</code></pre>
<p>to get the materials you will need for these exercises.</p>
<hr />
<h2 id="exercise-deploy-image-classification-as-a-web-service">Exercise:
Deploy image classification as a web service</h2>
<p>We will start by deploying an image classification model as a web
service. Users can upload an image to a basic web app (in Flask) in
their browser, then the app will:</p>
<ul>
<li>resize the image to the correct input dimensions</li>
<li>pass it as input to an image classification model that has been fine
tuned for food classification</li>
<li>and return the most likely class (and estimated confidence), as well
as the wall time of the <code>model.predict</code> call.</li>
</ul>
<h3 id="containerize-the-basic-web-app">Containerize the basic web
app</h3>
<p>To make it easy to deploy our basic application, we will containerize
it - package the source code together with an operating system, software
dependencies, and anything else it needs to run. This is a convenient
way to distribute the code along with everything it needs. This will
also make it much easier to deploy multiple copies of the application
(to handle heavier load) in future exercises.</p>
<p>The source code for our basic application is inside the
<code>app</code> subdirectory inside <code>k8s-ml</code>. It has the
following materials:</p>
<pre><code>-   app
    -   instance
    -   static
    -   templates
    -   app.py
    -   Dockerfile
    -   requirements.txt
    -   model.keras</code></pre>
<p>You can browse these files in the web interface <a
href="https://github.com/teaching-on-testbeds/k8s-ml/tree/gh-pages/app">here</a>.</p>
<p>Note that a saved model - <code>model.keras</code> - is inside this
directory. Then, inside <code>app.py</code>,</p>
<ul>
<li>when <code>app.py</code> runs, it loads the saved model:</li>
</ul>
<!-- -->
<pre><code>model = load_model(&quot;model.keras&quot;)</code></pre>
<ul>
<li>when a user uploads an images file to the app, or when a special
“test” path in the URL is used, a <code>model_predict</code> function is
called that returns the predicted class of the image:</li>
</ul>
<!-- -->
<pre><code>def model_predict(img_path, model):
    im = Image.open(img_path).convert(&#39;RGB&#39;)
    image_resized = im.resize(target_size, Image.BICUBIC)
    test_sample = np.array(image_resized)/255.0
    test_sample = test_sample.reshape(1, target_size[0], target_size[1], 3)
    classes = np.array([&quot;Bread&quot;, &quot;Dairy product&quot;, &quot;Dessert&quot;, &quot;Egg&quot;, &quot;Fried food&quot;,
    &quot;Meat&quot;, &quot;Noodles/Pasta&quot;, &quot;Rice&quot;, &quot;Seafood&quot;, &quot;Soup&quot;,
    &quot;Vegetable/Fruit&quot;])
    test_probs = model.predict(test_sample)
    most_likely_classes = np.argmax(test_probs.squeeze())
    
    return classes[most_likely_classes], test_probs.squeeze()[most_likely_classes]</code></pre>
<p>The <code>app</code> directory also includes a <a
href="https://github.com/teaching-on-testbeds/k8s-ml/blob/main/app/Dockerfile">Dockerfile</a>.
This file describes how to build a container for this application,
including:</p>
<ul>
<li>what “base” container should be used (we’ll use a Python 3.9 base
container)</li>
<li>what needs to run inside the container when it starts (we’ll install
the libraries listed in <code>requirements.txt</code> using
<code>pip</code>)</li>
<li>what files should be copied to the container (everything in
<code>app</code>)</li>
<li>and what command to run on the container when it is ready (we will
run <code>python app.py</code> to start our web app)</li>
</ul>
<p>We will use this Dockerfile to build the container (naming it
<code>ml-app</code>) and then push it to a local distribution “registry”
of containers (which is already running on node-0, on port 5000).</p>
<p>In an SSH session on “node-0”, run</p>
<pre><code>docker build -t ml-app:0.0.1 ~/k8s-ml/app
docker tag ml-app:0.0.1  node-0:5000/ml-app:0.0.1
docker push node-0:5000/ml-app:0.0.1</code></pre>
<h3 id="deploy-the-basic-web-app">Deploy the basic web app</h3>
<p>Now that we have containerized our application, we can run it! Let’s
run it now, and will indicate that we want incoming requests on port
32000 to be passed to port 5000 on the container (where our Flask
application is listening). In an SSH session on “node-0”, run</p>
<pre><code>docker run -d -p 32000:5000 node-0:5000/ml-app:0.0.1</code></pre>
<p>Here,</p>
<ul>
<li><code>-d</code> is for detach mode - so we can leave it running in
the background. (If you need to run a container in the foreground, for
debugging purposes, you will omit this argument.)</li>
<li><code>-p</code> is to assign the mapping between “incoming request
port” (32000) and “container port’ (5000).</li>
</ul>
<p>You can see the list of running containers with</p>
<pre><code>docker ps</code></pre>
<p>which will show containers related to the docker register and
Kubernetes deployment, but will also show one running container using
the <code>node-0:5000/ml-app:0.0.1</code> image. To restrict the output
to just this image, we can use</p>
<pre><code>docker ps -f ancestor=node-0:5000/ml-app:0.0.1</code></pre>
<p>Now we can visit our web service and try it out! Run this command on
“node-0” to get the URL to use:</p>
<pre><code>echo http://$(curl -s ifconfig.me/ip):32000</code></pre>
<p>Then, open your browser, paste this URL into the address bar, and hit
Enter.</p>
<p>When the web app has loaded, upload an image to your classification
service, and check its prediction.</p>
<p>You can also see the resource usage - in terms of CPU and memory - of
your container, with</p>
<pre><code>docker stats $(docker ps -q -f ancestor=node-0:5000/ml-app:0.0.1)</code></pre>
<p>(which uses command substitution to get the ID of any running
container using our <code>ml-app</code> image, then gets its
statistics). Use Ctrl+C to stop this live display.</p>
<p>When you are finished, you can stop the container by running:</p>
<pre><code>docker stop $(docker ps -q -f ancestor=node-0:5000/ml-app:0.0.1)</code></pre>
<p>(which uses command substitution to get the ID of any running
container using our <code>ml-app</code> image, then stops it).</p>
<h3 id="transfer-a-saved-model-to-the-remote-host">Transfer a saved
model to the remote host</h3>
<p>This is a “bring your own model” activity! You should have already
trained a model, saved it as <code>model.keras</code>, and downloaded
your saved model.</p>
<p>Use <code>scp</code> to transfer this file to
<code>~/k8s-ml/app</code> on your “node-0” host.</p>
<p>Then, repeat the steps in the “Containerize the basic web app” and
“Deploy the basic web app” sections, i.e. </p>
<pre><code>docker build -t ml-app:0.0.1 ~/k8s-ml/app
docker tag ml-app:0.0.1  node-0:5000/ml-app:0.0.1
docker push node-0:5000/ml-app:0.0.1</code></pre>
<p>and then</p>
<pre><code>docker run -d -p 32000:5000 node-0:5000/ml-app:0.0.1</code></pre>
<blockquote>
<p><strong>Debugging your service</strong>: if something goes wrong, you
can use <code>docker run -p 32000:5000 node-0:5000/ml-app:0.0.1</code>
to run your service <em>without</em> detaching from it, so that you can
see output (including error messages).</p>
</blockquote>
<p>(For a model that is very large, it may take a few minutes - even up
to 10 minutes - before the container is ready to accept requests.)</p>
<p>Use your model to classify an image. Make sure your “deployed” model
returns the same result for your custom test image as it did in the
Colab notebook. Also note the inference time (the first inference may
take much longer than subsequence predictions, so discard the first
result.)</p>
<p>Also check the resource usage during inference with</p>
<pre><code>docker stats $(docker ps -q -f ancestor=node-0:5000/ml-app:0.0.1)</code></pre>
<p>When you are finished, stop the container by running:</p>
<pre><code>docker stop $(docker ps -q -f ancestor=node-0:5000/ml-app:0.0.1)</code></pre>
<h2
id="exercise-deploy-your-service-using-container-orchestration-kubernetes">Exercise:
Deploy your service using container orchestration (Kubernetes)</h2>
<p>In the previous exercise, we deployed a container by running it
directly. Now, we will deploy the same container using Kubernetes, a
platform for container orchestration.</p>
<p>What are some benefits of using a container orchestration framework
like Kubernetes, rather than deploying containers directly?</p>
<ul>
<li><em>Container Orchestration:</em> Kubernetes helps to automate the
deployment, scaling and management of containers.</li>
<li><em>Self-healing:</em> If a container fails, Kubernetes can
automatically detect it and replace it with a functional container.</li>
<li><em>Load balancing:</em> Kubernets can distribute traffic for an
application across multiple instances of running containers. (We’ll try
this in the next exercise.)</li>
<li><em>Resource management:</em> Kubernetes allows you to set resource
limits for containers, to ensure that the application has the resources
to run efficiently.</li>
</ul>
<p><em>Pods</em> are the basic components in Kubernetes, and are used to
deploy and manage containerized applications in a scalable and efficient
way. They are designed to be ephemeral, meaning they can be created,
destroyed, and recreated as needed. They can also be replicated, which
allows for load balancing and high availability.</p>
<p>Although we will eventually deploy pods across all three of our
“worker” nodes, our deployment will be managed from the “controller”
node, which is “node-0”.</p>
<p>To deploy an app on a Kubernetes cluster, we use a manifest file,
which describes our deployment. For this exercise, we will use the
“deployment_k8s.yaml” file inside the “~/k8s-ml/deploy_k8s” directory,
which you can see <a
href="https://github.com/teaching-on-testbeds/k8s-ml/blob/main/deploy_k8s/deployment_k8s.yaml">here</a>.</p>
<p>This manifest file defines a Kubernetes service named
“ml-kube-service” and a Kubernetes deployment named “ml-kube-app”.</p>
<ul>
<li>Inside the service definition, we create a service of
<code>type: NodePort</code>. This service passes incoming requests on a
specified port, to a (different) port on the pod.</li>
<li>Inside the deployment definition, you can see that
<ul>
<li>the “ml-app” container you built earlier will be retrieved from the
local registry (“node-0:5000/ml-app:0.0.1”),</li>
<li>the deployment will include just a single copy of our pod
(“replicas: 1”).</li>
<li>there is a “readiness” probe defined - the container is considered
“Ready” and requests will be forwarded to it only when it responds with
a success code to 3 HTTP requests on the <code>/test</code>
endpoint.</li>
</ul></li>
</ul>
<p>It also defines the resource requirements of the container, in terms
of CPU cores and memory. The “request” defines the minimum resource a
container may get, and the “limit” defines the maximum resource a
container may get.</p>
<p>To start this deployment, we will run:</p>
<pre><code>kubectl apply -f ~/k8s-ml/deploy_k8s/deployment_k8s.yaml</code></pre>
<p>and make sure the following output appears:</p>
<pre><code>service/ml-kube-service created
deployment.apps/ml-kube-app created</code></pre>
<p>Let’s check the status of the service. Run the command:</p>
<pre class="shell"><code>kubectl get svc -o wide</code></pre>
<p>The output will include a line similar to</p>
<pre><code>NAME                 TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE     SELECTOR
ml-kube-service   NodePort    10.233.37.25   &lt;none&gt;        6000:32000/TCP   12m     app=ml-kube-app</code></pre>
<p>It may take a few minutes for the pod to start running. To check the
status of pods, run:</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>The output may include a line similar to</p>
<pre><code>NAME                                               READY   STATUS              RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
ml-kube-app-7b4c8648c6-r8zvv                    0/1     ContainerCreating   0          22s     &lt;none&gt;        node-2   &lt;none&gt;           &lt;none&gt;</code></pre>
<p>In this example, the status of the pod is
<code>ContainerCreating</code>, which means the container is getting
ready. When it reaches the <code>Running</code> state, then it means the
pod is healthy and is running. When it shows “1/1” in the “Ready”
column, it is ready to accept requests according to the probe we had set
up.</p>
<p>(As before, if your model is large, it may take a while before it is
ready to accept requests.)</p>
<p>Once the pod is ready, check the resource usage (CPU and memory) of
the pod with</p>
<pre><code>kubectl top pod</code></pre>
<p>Note that the resource usage varies depending on whether or not the
pod is currently serving a request!</p>
<p>Get the URL of the service - run</p>
<pre><code>echo http://$(curl -s ifconfig.me/ip):32000</code></pre>
<p>copy and paste this URL into your browser’s address bar, and verify
that your app is up and running there.</p>
<h3 id="test-deployment-under-load">Test deployment under load</h3>
<p>To test the load on the deployment we will use <a
href="https://linux.die.net/man/1/siege">siege</a>, a command-line tool
used to test and analyze the performance of web servers. It can generate
a significant amount of traffic to test the response of a web server
under load.</p>
<p>Install Siege on node-0:</p>
<pre><code>sudo apt-get update; sudo apt-get -y install siege</code></pre>
<p>Open a second SSH session on node-0. In one, run</p>
<pre><code>watch -n 5 kubectl top pod</code></pre>
<p>to monitor the pod’s resource usage in real time (This will be
updated every 5 seconds). In the second SSH session, run</p>
<pre><code>siege -c 10 -t 30s http://$(curl -s ifconfig.me/ip):32000/test</code></pre>
<p>Here Siege will generate traffic to a “test” endpoint on your
website, which requests a prediction for a pre-saved image, for 30
seconds with a concurrency level of 10 users. After it finishes
execution, make a note of key results - how many transactions were
served successfully, how many failed, the transaction rate, and what the
average response time was (note that this includes inference time, as
well as several other elements).</p>
<h3 id="stop-the-deployment">Stop the deployment</h3>
<p>When you are done with your experiment, make sure to delete the
deployment and service. To delete, run the command:</p>
<pre><code>kubectl delete -f ~/k8s-ml/deploy_k8s/deployment_k8s.yaml</code></pre>
<p>and look for output like</p>
<pre><code>service &quot;ml-kube-service&quot; deleted
deployment.apps &quot;ml-kube-app&quot; deleted</code></pre>
<p>Use</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>and verify that (eventually) no pods are running your app.</p>
<h2 id="exercise-deploy-your-service-with-load-balancing">Exercise:
Deploy your service with load balancing</h2>
<p>In the previous exercise, we deployed a single replica of a
Kubernetes pod. But if the load on the service is high, the single pod
will have slow response times. We can address this by deploying multiple
“replicas” of the pod, and distributing the traffic across them by
assigning each incoming request to a pod. This is called <strong>load
balancing</strong>.</p>
<p>The manifest file for deploying a load balanced service is named
“deployment_lb.yaml”, and it is inside the “~/k8s-ml/deploy_lb”
directory. You can see it <a
href="https://github.com/teaching-on-testbeds/k8s-ml/blob/main/deploy_lb/deployment_lb.yaml">here</a>.</p>
<p>This manifest file defines a Kubernetes service of type
<code>LoadBalancer</code> with the name “ml-kube-service” and a
Kubernetes deployment named “ml-kube-app”. There are two major
differences between this deployment and the previous deployment:</p>
<ul>
<li>in this one, we specify that the service is of
<code>type: LoadBalancer</code>, i.e. instead of directly passing
incoming requests to one pod, will place a load balancer service in
“front” of the pods that will distribute the requests across pods.</li>
<li>in this one, we specify <code>replicas: 5</code> where previously we
used <code>replicas: 1</code>.</li>
</ul>
<p>To start this deployment, we will run:</p>
<pre><code>kubectl apply -f ~/k8s-ml/deploy_lb/deployment_lb.yaml</code></pre>
<p>and make sure the following output appears:</p>
<pre><code>service/ml-kube-service created
deployment.apps/ml-kube-app created</code></pre>
<p>It will take a few minutes for the pods to start running. To check
the status of deployment, run:</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>and wait until the output shows that all pods are in the “Running”
state and show as “1/1” in the “Ready” column. Note that the pods will
be deployed across all of the nodes in the cluster. Also note that if
some pods are “Ready” but not others, requests will be sent only to the
pods that are “Ready”.</p>
<p>Once the pods are ready, you can see the resource usage (CPU and
memory) of the pods with</p>
<pre><code>kubectl top pod</code></pre>
<p>Get the URL of the service - run</p>
<pre class="shell"><code>echo http://$(curl -s ifconfig.me/ip):32000</code></pre>
<p>copy and paste this URL into your browser’s address bar, and verify
that your app is up and running there.</p>
<h3 id="test-deployment-under-load-1">Test deployment under load</h3>
<p>As before, we will test this deployment under load. Open a second SSH
session on node-0. In one, run</p>
<pre><code>watch -n 5 kubectl top pod</code></pre>
<p>to monitor the pod’s resource usage in real time (This will be
updated every 5 seconds). In the second SSH session, run</p>
<pre><code>siege -c 10 -t 30s http://$(curl -s ifconfig.me/ip):32000/test</code></pre>
<p>After it finishes execution, make a note of key results - how many
transactions were served successfully, how many failed, the transaction
rate, and what the average response time was (note that this includes
inference time, as well as several other elements).</p>
<h3 id="stop-the-deployment-1">Stop the deployment</h3>
<p>When you are done with your experiment, make sure to delete the
deployment and service. To delete, run the command:</p>
<pre><code>kubectl delete  -f ~/k8s-ml/deploy_lb/deployment_lb.yaml</code></pre>
<p>and look for output like</p>
<pre><code>service &quot;ml-kube-service&quot; deleted
deployment.apps &quot;ml-kube-app&quot; deleted</code></pre>
<p>Use</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>and verify that (eventually) no pods are running your app.</p>
<h2 id="exercise-deploy-a-service-with-dynamic-scaling">Exercise: Deploy
a service with dynamic scaling</h2>
<p>When we used load balancing to distribute incoming traffic across
multiple pods, the response time under load was much faster. But during
time intervals when the load is not heavy, it may be wasteful to deploy
so many pods. (The application is loaded and uses memory and some CPU
even when there are no requests!)</p>
<p>To address this issue, we can use scaling - where the resource
deployment changes in response to load on the service. In this exercise,
specifically we use <strong>horizontal scaling</strong>, which adds more
pods/replicas to handle increasing levels of work, and removes pods when
they are not needed. (This is in contrast to <strong>vertical
scaling</strong>, which would increase the resources assigned to pods -
CPU and memory - to handle increasing levels of work.)</p>
<p>The manifest file for deploying a service with scaling is
“deployment_hpa.yaml”, and it is inside the “~/k8s-ml/deploy_hpa”
directory. You can see it <a
href="https://github.com/teaching-on-testbeds/k8s-ml/blob/main/deploy_hpa/deployment_hpa.yaml">here</a>.</p>
<p>There are two differences between this deployment and the previous
deployment:</p>
<ul>
<li>in this one, we add a service of
<code>type: HorizontalPodAutoscaler</code>. We specify the minimum
number of replicas and maximum number of replicas we want to have in our
deployment, and the condition under which to increase the number of
replicas. (This <code>HorizontalPodAutoscaler</code> service is <em>in
addition to</em> the <code>LoadBalancer</code> service, which is also in
place.)</li>
<li>in this one, we specify <code>replicas: 1</code> again in the
deployment - the initial deployment has 1 replica, but it may be scaled
up to 5 by the autoscaler.</li>
</ul>
<p>To start this deployment, we will run:</p>
<pre><code>kubectl apply -f ~/k8s-ml/deploy_hpa/deployment_hpa.yaml</code></pre>
<p>Let’s check the status of the service. Run the command:</p>
<pre><code>kubectl get svc -o wide</code></pre>
<p>and then</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>Initially, you will see one pod in the deployment. Wait until the pod
is “Running” and has a “1/1” in the “Ready” column.</p>
<p>Get the URL of the service - run</p>
<pre><code>echo http://$(curl -s ifconfig.me/ip):32000</code></pre>
<p>copy and paste this URL into your browser’s address bar, and verify
that your app is up and running there.</p>
<h3 id="test-deployment-under-load-2">Test deployment under load</h3>
<p>You will need two SSH sessions on node-0. In one, run</p>
<pre><code>kubectl get hpa --watch</code></pre>
<p>to see the current state of the autoscaler.</p>
<p>In the second SSH session, run</p>
<pre><code>siege -c 10 -t 360s http://$(curl -s ifconfig.me/ip):32000/test</code></pre>
<p>Note that this test is of a longer duration, so that you will have
time to observe additional replicas being brought up and becoming ready
to use.</p>
<h3 id="stop-the-deployment-2">Stop the deployment</h3>
<p>When you are done with your experiment, make sure to delete the
deployment and services. To delete, run the command:</p>
<pre><code>kubectl delete -f  ~/k8s-ml/deploy_hpa/deployment_hpa.yaml</code></pre>
<p>Use</p>
<pre><code>kubectl get pods -o wide</code></pre>
<p>and verify that (eventually) no pods are running your app.</p>
</body>
</html>
